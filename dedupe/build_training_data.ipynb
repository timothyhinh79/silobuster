{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22e302a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tld'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45478/3719581001.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostgres_feed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPostgresFeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmangle_org_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_url\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmangle_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtld_swap_prob_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtld_swap_prob_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hackathon/microservice/source/silobuster-dedupe/manglers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtld_swap_prob_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hackathon/microservice/source/silobuster-dedupe/manglers/mangle_url.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtld\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tld\u001b[0m \u001b[0;31m# to get domain extension (aka top-level domain or 'TLD') of URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmanglers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmangle_org_name\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_remove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_replace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_null\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# consider getting more extensive list of domain extensions, from data or from TLD website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tld'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "from psycopg2 import connect, extras\n",
    "\n",
    "from libs.connectors.postgres_connector import PostgresConnector\n",
    "from libs.feeds.postgres_feed import PostgresFeed\n",
    "\n",
    "from manglers.mangle_org_name import mangle_org_name\n",
    "from manglers.mangle_url import mangle_url\n",
    "from manglers.tld_swap_prob_dict import tld_swap_prob_dict\n",
    "\n",
    "# connection details for source data table\n",
    "source_host = 'silobuster-db-do-user-12298230-0.b.db.ondigitalocean.com'\n",
    "source_user = 'jameyc'\n",
    "source_passwd = 'UXZSXXXSFZeU8XKw'\n",
    "source_db = 'defaultdb'\n",
    "source_port = 25060\n",
    "\n",
    "# connection details for training data table\n",
    "training_host = 'silobuster-db-do-user-12298230-0.b.db.ondigitalocean.com'\n",
    "training_user = 'jameyc'\n",
    "training_passwd = 'UXZSXXXSFZeU8XKw'\n",
    "training_db = 'jameycdb'\n",
    "training_port = 25060\n",
    "training_table = 'organizations_normalized'\n",
    "\n",
    "csv_output = 'training_set1.csv'\n",
    "\n",
    "# some of below probabilities are rough estimates based on organization table\n",
    "# e.g. around 3% of non-blank URLs in organization table are missing scheme\n",
    "url_mangling_probs_dict = {\n",
    "    'www_remove_prob': .22, \n",
    "    'scheme_remove_prob': .03, \n",
    "    'remove_s_from_https_prob': .68,\n",
    "    'append_extra_slash_prob': .35,\n",
    "    # 'change_domain_ext_prob': .05, # arbitrary probability\n",
    "    'mispell_remove_char_prob': .02, # arbitrary probability\n",
    "    'mispell_replace_char_prob': .02, # arbitrary probability\n",
    "    'mispell_null_url_prob': .02 # arbitrary probability\n",
    "}\n",
    "\n",
    "print ('Starting connections...')\n",
    "source_conn = connect(\n",
    "        database=source_db,\n",
    "        user=source_user,\n",
    "        password=source_passwd,\n",
    "        host=source_host,\n",
    "        port=source_port\n",
    "    )\n",
    "\n",
    "training_conn = connect(\n",
    "        database=training_db,\n",
    "        user=training_user,\n",
    "        password=training_passwd,\n",
    "        host=training_host,\n",
    "        port=training_port\n",
    "    )\n",
    "\n",
    "select_qry = \"select t1.name, t1.description, t1.url, t3.address_1, t3.address_2, t3.city, t3.region, t3.state_province, t3.postal_code, t3.country, t3.type from organization t1 left join location t2 on t1.id = t2.organization_id left join address t3 on t3.location_id = t2.id\"\n",
    "insert_qry = f\"INSERT INTO {training_table} (name, description, url, address_1, address_2, city, state_province, postal_code, country, type, region) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id\"\n",
    "insert_dup_qry = f\"INSERT INTO {training_table} (name, description, url, address_1, address_2, city, state_province, postal_code, country, type, region, duplicate_id, duplicate_type, training_set) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "print (\"Connected?\")\n",
    "dups = []\n",
    "\n",
    "# Data keys for mangling\n",
    "address_keys = []\n",
    "with open('./helper_data/street_suffixes.csv', 'r') as suffixes:\n",
    "    csv_reader = csv.reader(suffixes)\n",
    "    for row in csv_reader:\n",
    "        row_clean = [suffix for suffix in row if suffix] # removing empty strings\n",
    "        address_keys.append(row_clean)\n",
    "\n",
    "state_keys = list()\n",
    "state_keys.append(['wa', 'wash', 'washington'])\n",
    "\n",
    "with source_conn.cursor(cursor_factory=extras.RealDictCursor) as source_cur:\n",
    "    source_cur.execute(select_qry)\n",
    "    data = source_cur.fetchall()\n",
    "    print (\"Retrieved data...\")\n",
    "    \n",
    "    dup_address = random.randint(1,3)\n",
    "    dup_blank_stuff = random.randint(1,2)\n",
    "    \n",
    "    for count, row in enumerate(data):\n",
    "        \n",
    "        print (f\"Insert row count: {count}\")\n",
    "\n",
    "                \n",
    "        name = row['name'].strip().lower()\n",
    "        try:\n",
    "            desc = row['description'].strip().lower()\n",
    "        except:\n",
    "            desc = ''\n",
    "        try:\n",
    "            url = row['url'].strip().lower()\n",
    "        except:\n",
    "            url = ''\n",
    "        try:\n",
    "            address_1 = row['address_1'].strip().lower()\n",
    "        except:\n",
    "            address_1 = ''\n",
    "        try:\n",
    "            address_2 = row['address_2'].strip().lower()\n",
    "        except:\n",
    "            address_2 = ''\n",
    "        try:\n",
    "            city = row['city'].strip().lower()\n",
    "        except:\n",
    "            city = ''\n",
    "        try:\n",
    "            region = row['region'].strip().lower()\n",
    "        except:\n",
    "            region = ''\n",
    "        try:\n",
    "            state = row['state_province'].strip().lower()\n",
    "        except:\n",
    "            state = ''\n",
    "        try:\n",
    "            postal = row['postal_code'].strip().lower()\n",
    "        except:\n",
    "            postal = ''\n",
    "        try:\n",
    "            country = row['country'].strip().lower()\n",
    "        except:\n",
    "            country = ''\n",
    "        try:\n",
    "            type_row = row['type'].strip().lower()\n",
    "        except:\n",
    "            type_row = ''\n",
    "        \n",
    "        # Write the rows to the normalized table\n",
    "        with training_conn.cursor() as training_cur:\n",
    "            training_cur.execute(insert_qry, [\n",
    "                    name, \n",
    "                    desc, \n",
    "                    url, \n",
    "                    address_1, \n",
    "                    address_2, \n",
    "                    city, \n",
    "                    state, \n",
    "                    postal,\n",
    "                    country, \n",
    "                    type_row,\n",
    "                    region, \n",
    "\n",
    "            ])\n",
    "            insert_id = training_cur.fetchone()[0]\n",
    "            training_conn.commit()\n",
    "        \n",
    "        \n",
    "        with open(csv_output, 'a') as csvfile:\n",
    "            w = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "            w.writerow([\n",
    "                name,\n",
    "                desc,\n",
    "                url,\n",
    "                address_1,\n",
    "                address_2,\n",
    "                city,\n",
    "                state,\n",
    "                postal,\n",
    "                country,\n",
    "                type_row,\n",
    "                region,\n",
    "                '',\n",
    "                '',\n",
    "                0,\n",
    "            ])\n",
    "\n",
    "        #create a duplicate/mangled name\n",
    "        mangled_name = mangle_org_name(\n",
    "            name, \n",
    "            remove_prob = 0.02,\n",
    "            replace_prob = 0.02,\n",
    "            null_prob = 0.02\n",
    "        )\n",
    "\n",
    "        duplicate_type = []\n",
    "        if mangled_name != name:\n",
    "            duplicate_type.append('mangled_name')\n",
    "\n",
    "        mangled_url = mangle_url(url, url_mangling_probs_dict, tld_swap_prob_dict)\n",
    "        if mangled_url != url:\n",
    "            duplicate_type.append('mangled_url')\n",
    "\n",
    "        # initializing mangled fields as the original values - this will allow us to identify if these fields were mangled later on\n",
    "        # when deciding if we need to insert another \"duplicate\" row\n",
    "        mangled_address_1 = address_1\n",
    "        mangled_desc = desc\n",
    "        mangled_region = region\n",
    "        mangled_country = country\n",
    "\n",
    "        #create a duplicate address\n",
    "        if dup_address == count:\n",
    "            dup_address = random.randint(1, 3) + count\n",
    "            duplicate_type.append('street_dup')\n",
    "            lst = address_1.split(' ')\n",
    "            for a in address_keys:\n",
    "                if lst[len(lst)-1] in a:\n",
    "                    new_street = a[random.randint(0, len(a)-1)]\n",
    "                    del lst[len(lst)-1]\n",
    "                    lst.append(new_street)\n",
    "                    break\n",
    "                    \n",
    "            mangled_address_1 = ' '.join(lst)\n",
    "            if count // dup_blank_stuff == 0:\n",
    "                duplicate_type.append('blanked')\n",
    "                mangled_desc = ''\n",
    "                mangled_region = ''\n",
    "                mangled_country = ''\n",
    "\n",
    "        # if any of the fields were mangled, create a duplicate row in database and in training set CSV file\n",
    "        if mangled_name != name or mangled_address_1 != address_1 or mangled_desc != desc or mangled_url != url or mangled_region != region or mangled_country != country:  \n",
    "            dup_row = {\n",
    "                'name': mangled_name,\n",
    "                'description': mangled_desc,\n",
    "                'url': mangled_url,\n",
    "                'address_1': mangled_address_1,\n",
    "                'address_2': address_2,\n",
    "                'city': city,\n",
    "                'region': mangled_region,\n",
    "                'state': state,\n",
    "                'postal': postal,\n",
    "                'country': mangled_country,\n",
    "                'duplicate_id': insert_id,\n",
    "                'duplicate_type': duplicate_type,\n",
    "                'training_set': 1\n",
    "            }\n",
    "\n",
    "            # name, description, url, address_1, address_2, city, state_province, postal_code, country, type, region, duplicate_id, duplicate_type, training_set\n",
    "            \n",
    "            with training_conn.cursor() as dup1_cur:\n",
    "                dup1_cur.execute(insert_dup_qry, [\n",
    "                        mangled_name,\n",
    "                        mangled_desc,\n",
    "                        mangled_url,\n",
    "                        mangled_address_1,\n",
    "                        address_2,\n",
    "                        city,\n",
    "                        state,\n",
    "                        postal,\n",
    "                        mangled_country,\n",
    "                        type_row,\n",
    "                        mangled_region,\n",
    "                        insert_id,\n",
    "                        '|'.join(duplicate_type),\n",
    "                        1,\n",
    "                    ])\n",
    "                \n",
    "    \n",
    "\n",
    "            with open(csv_output, 'a') as csvfile:\n",
    "                w = csv.writer(csvfile, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "                w.writerow([\n",
    "                    mangled_name,\n",
    "                    mangled_desc,\n",
    "                    mangled_url,\n",
    "                    mangled_address_1,\n",
    "                    address_2,\n",
    "                    city,\n",
    "                    state,\n",
    "                    postal,\n",
    "                    mangled_country,\n",
    "                    type_row,\n",
    "                    mangled_region,\n",
    "                    insert_id,\n",
    "                    '|'.join(duplicate_type),\n",
    "                    1,\n",
    "                ])\n",
    "                \n",
    " \n",
    "print ('finished')           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085c4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
